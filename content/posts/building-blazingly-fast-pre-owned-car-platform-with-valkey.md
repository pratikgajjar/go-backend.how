+++
title = "Building blazingly fast pre-owned car platform with Valkey - Part 1"
description = "You will be able to use valkey like swiss-knife it is to serve web pages at blazingly fast speed."
date = 2024-06-16T23:27:27+05:30
lastmod = 2024-06-16T23:27:27+05:30
tags = ['Django', 'Redis', 'DRF', 'CDC']
draft = false
images = []
+++

**Introduction**

Welcome! This blog post is a work in progress, but feel free to dive in. The content will cover how to leverage [Valkey](https://valkey.io/) for serving web pages at blazingly fast speeds. This guide assumes you have some familiarity with backend technology.

{{< details "What is Valkey?" >}}
Valkey is an open-source, high-performance key/value datastore licensed under BSD. It supports a variety of workloads such as caching, message queues, and can function as a primary database. Valkey can operate as a standalone daemon or in a cluster with options for replication and high availability.

{{</details>}}
It originated as a fork of Redis after Redis changed its license terms. More details on the forking issue can be found [here](https://arstechnica.com/information-technology/2024/04/redis-license-change-and-forking-are-a-mess-that-everybody-can-feel-bad-about/).

# What ?

Our platform for pre-owned cars allows buyers to access detailed reports, schedule test drives, and make purchase decisions. It combines a marketplace and inventory model.

## Marketplace

We provide a platform for buyers and sellers to communicate directly. Revenue is generated by facilitating buyer-seller interactions and charging for contact details.

## Inventory

We own the cars and provide test drives at various locations. Revenue comes from higher margins on refurbished cars.

## User flow

The typical user journey involves

1. Viewing ads
2. Generating a lead by entering a mobile number
3. Booking a visit on the platform
4. Test driving at a store
5. Making a purchase decision

Our goal is to maximize lead generation by providing a fast and seamless user experience.

# Why ?

- Aim to achieve a 60ms response time for critical entry points, a target set by our co-founder.
- 95%+ traffic comes from ads on Google, Facebook, Instagram, leading users to either a detailed car page or a listing page.

1. Dedicated Page - Complete details of a single car, featuring over 10 photos and additional information.
2. Listing Page - A catalog of cars with basic information and about four photos that can be scrolled horizontally.

> Amazon found that every 100ms of latency cost them 1% in sales.
In 2006, Google found an extra .5 seconds in search page generation
time dropped traffic by 20%. - [Marissa Mayer](http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html)

# Specifications

- Up to 10,000 active cars available for users to view.
- Listing page shows cars ordered by the [LightFM](https://github.com/lyst/lightfm) model.
- Filters available based on car attributes.
- Handles both anonymous and logged-in users for personalized recommendations.

# Exisitng design

- Frontend: React Progressive Web App (PWA) built using [create-react-app](https://create-react-app.dev/) as the frontend.
- Backend: REST API implemented via [Django](https://www.djangoproject.com/) and Django-Rest-Framework.

## Frontend

PWAs load quickly after the first visit, but the initial load can be slow, especially on low-end devices. However, once loaded, they offer smooth navigation and offline access. We aim to make pages load fast right from the first visit, even on low-end devices.

## Backend

The request-response lifecycle involves multiple steps, with significant time spent on model serialization and database (MySQL) calls.

### Dedicated Page

```md
GET https://api.car.com/listing/14006824/
```

Fetches car data based on an ID in URI and returns JSON.

Dynamic Entities

- seller_mobile_no - User got the seller no, paid feature.
- is_seen - User has seen the
- test_drive_booked - have booked test drive for the given car
- inspection_report - User can access full report, paid feature.
- offer_amount - buyer can choose to offer lower amount than listed amount
- interested_people - How many users made bid, or scheduled test drive
- images - List of image URLs, can change after car made live.
- price - Everyday prices may change based on demand-supply.
- rating - Provided by platform based on rules based logic depends on factor - owner, usage, damage

### Listing Page

```md
1. GET https://api.car.com/listing/?city_id=1&model_id=123&owner=1
2. GET https://api.car.com/listing/?slug=used-tata-nexon-cars-in-mumbai
3. GET https://api.car.com/listing/?slug=used-tata-nexon-cars-in-mumbai&buyer_id=123
4. GET https://api.car.com/listing/?slug=used-tata-cars-in-mumbai&seen_card_ids=10,11,12
```

Data science model can use either `buyer_id` or `seen_car_ids` to order list of cars based on most relevant to least. In case of `None` order based on generic recommendation.

```python
def get_recommendation(active_car_ids: [int], buyer_id: int, seen_card_ids: Optional[int]):
    return model.get_order(active_car_ids, buyer_id=buyer_id, seen_card_ids=seen_card_ids)
```

#### Filters

Filter By - Car colour, Make, Model, Accessory, Rating, Ownership etc.

Implemented with [Django-filter](https://django-filter.readthedocs.io/en/stable/guide/usage.html), complex queries with multiple joins can strain the database.

#### Pagination

We can return list of all cars in mumbai city in one request, but that would put strain on all components involved.

For example for 1000 cars in a city

1. Database - Needs to fetch details of thousand cars
2. Backend server - Convert database response into `JSON`, this would cause high CPU usage
3. Client - Needs to generate `HTML` for thousand cars, browser need to paint the screen.

This would case issues at each stage, to avoid this we return only few set of cars in one request. Among cursor, limit-offset and page-no pagination options we used page-no pagination.

**Page No pagination**

Request

1. `page_size` - client decides how many cars to show, desktop users have more real estate allows to show more cars.
2. `page_no` - based on user's position, client sends page no.

Response

1. `count` - Maximum no of pages client can ask for, after this number client shows `No more results`
2. `next_page` - URL to fetch next page
3. `prev_page` - URL to fetch previous page
2. `results` - List of `JSON` containing basic car information

How do we decide among 2 cars, which one should come before which one ?

To decide that, client can send query param - `order_by` - in backend based on this we order the pages.

Order options

1. reco - Most relevant to least - default order - based on data science model
2. newest - Last added car comes first
3. price_asc - Ascending order of price
4. price_desc - Descending order of price

```md
GET https://api.car.com/listing/?city_id=1&order_by=reco&page_no=2&page_size=10
```

Here for recommendation we would be generating order of cars during runtime with each request.

# Achieving the 60ms response time

## Optimize Frontend

### How Does Client-Side Rendering Work?

Client-side rendering (CSR) involves the frontend receiving a JSON payload and using JavaScript to create HTML for displaying data. Libraries like React handle this efficiently by checking if the DOM tree needs updating and only modifying the parts that have changed. However, this process can be slow on low-end devices because React has to generate the HTML, perform a diff, and then update DOM.

### Why Not Send HTML Directly?

Sending pre-rendered HTML, which browsers are optimized to handle, can be more efficient. This is where Next.js (year 2019), becomes valuable. By running a Node server on the backend, Next.js can send HTML for the initial page load. After this, user interactions such as navigating between pages or viewing details fall back to client-side rendering. This hybrid approach offers a smooth, app-like experience.

## Optimize Backend

North star for us would be avoiding duplicate work and reducing runtime computation via storing pre-computed results.

### Duplicate work

1. Creating `JSON` for same car with each request
2. Computing order for recommendations for anon users
3. Computing order of cars for each page for given user or set of seen cars

### Pre-compute

1. Dynamic part of `JSON` that only depends on car.
2. Store `Counters` and `Boolean` to avoid DB calls.

- no_of_buyers_shown_interest
- is_seen
- is_car_visit_scheduled

Consider these points:

- Daily automatic price changes for cars.
- Operations team can add new cars at any time.
- Rendering full car data involves around 10 tables in a normalized schema.
- Database changes are done through various portals directly.
  - Operations portal (CakePHP app) for general operations.
  - Inventory App communicating with a Django application.
  - Data Science Python scripts handle bulk price updates.

### Cache Layer

#### Optimize Dedicated Page

Anonymous users

1. Store pre-computed `JSON` response of car
2. Fetch `JSON` by `key` and return response

```JavaScript
// ValKey (Redis) Data model 
key_pattern = "dp:{card_id}"
expiry = 12 hours
// value = gziped json payload
"dp:2323" : '{"id":2323, "images": ["url1", "url2", "url3"], "owner": "2nd", "inspection_report": ...}'
```

Compress `JSON` payload using GZip before storing the requests [django-redis](https://github.com/jazzband/django-redis).

```py
import gzip
CACHES = {
    "default": {
        # ...
        "OPTIONS": {
            "COMPRESSOR": "django_redis.compressors.gzip.GzipCompressor",
        }
    }
}
```

Logged in users

Computing results at runtime as response had many dynamic entities. Since most users would come from marketing campaigns, we wanted to optimize their experience first.

Are we done ?

> There are only two hard things in Computer Science: cache invalidation and naming things.
-- [Phil Karlton](https://martinfowler.com/bliki/TwoHardThings.html)

Not yet.

#### Cache Invalidation

We must invalidate the cache whenever changes occur in database that impacts the JSON response.
Otherwise, users may encounter stale results, leading to undefined behavior and customer dissatisfaction.

Let's say to compute `JSON` response for given car we use information from below tables

1. cars (`id`, `varient_id`, `color_id`, `owner`, `status`) - main car model
2. car_images (`car_id`, `image_url`)
3. car_features (`card_id`, `feature_id`)

How do we get notified each time a change occurs in above tables ?

Utilise

1. Db model abstraction provided in MVC frameworks. Here we can add logic in `save` method of model or use [signals](https://docs.djangoproject.com/en/5.0/topics/signals/)
2. Architectural pattern like Domain-Driven-Design to add logic that invalidates cache
3. How database replica functions, we can attach program that acts like replica and get changes.

We went with 3rd approach as there were many sources which could update the database, and adding
cache invalidation logic would become error prone and pollute other business logic.

We can listen to database changes on these 3 tables to invalidate the result stored in cache against given `card_id`.

In MySQL, via listening to binlog [[1]](#WAL) and we can get notified about all changes to the database - DDL & DML

```md
- Data defiantion language
  - CREATE | ALTER | DROP TABLE
- Data modification language
  - INSERT | UPDATE | DELETE | REPLACE FROM TABLE
```

Using [python library](https://github.com/pratikgajjar/mysql-data-stream-kafka) we attached program to listen mysql change log, and emitted changes to kafka topics.

A kafka topic is similar to log.

> What Is a Log?
> A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time. - [Jay Kreps, Linkedin](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)

```md
# Topic Name:
  {database_name}.{table_name}
# Messages:
   op: insert | delete | update
   before: json payload - column values before update
      with
        key = column name
        value = column value
   after: json payload - column values after update
```

Program would listen to kafka topics that can affect the response of dedicated page and rebuilds or invalidates the cache.

### Infrastructure

We introduced below components.

1. Consumer node - to listen changes and update cache
2. Producer node - to listen mysql bin log and push changes to kafka
3. ValKey server - to store cache data in-memory
4. Kafka-cluster - to store database change log
5. MySQL database - existing component

Now we have distributed systems, there comes fallacies.

> 1. The network is reliable; - [Wiki](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)

#### Failure of consumer node

Here we are listening to kafka topic, we will only commit when we have processed the change to build / invalidate the cache.
This allows us to do at-least once processing.

#### Failure of producer node

Here we have network partition, MySQL Server X Kafka X Producer Node. Worker reads from MySQL change log and pushes changes into kafka.
It reads one change from MySQL but was not able to push into Kafka due to failure of worker node or kafka. Would we lose the change forever ?

Nope, MySQL provides bin-log position with each change, so when along with row change we will commit bin-log position into kafka topic with the message.

Now, in case of failure of kafka or producer node, when we start reading bin-log next time, check the latest committed bin-log position from kafka topic
and use that to resume bin-log reading.

#### Failure of val-key server

We used managed elastic-cache in AWS, but for self-hosted approach need to ensure we are running valkey in cluster mode and have auto-failover setup in place.
Here consumer node will be unable to invalidate the payload causing consumer lag for the given topic, as soon as server becomes available consumer will start
clearing the backlog.

Exceeding the memory ?

For 10_000 active cars, with payload size of 20K for a dedicated page response we would consume only 2GB Memory.
Use volatile-lru policy for key eviction in case of max-memory, since we have TTL set on the key server will keep responding.

#### Failure of Kafka-cluster

Kafka provides multiple ways to handle the failure, start with no of brokers >= 3 and topic has replication factor of >=2 or how resilient system you want have.
Higher replication factor would cause higher disk space and network bandwidth.

#### Failure of MySQL

MySQL stores bin-log in disk so restart or fail to connect to server would not cause as long as it bin-log is not being cleared. Here producer will resume
from where it left reading the bin-log.

Note: Add alarm to watch bin-log size, as in event of prolonged producer failure bin-log could occupy full disk space causing failure device storage full errors.

Lost bin-log ?
We have added management command in django to rebuild the cache for all active cars.

### Optimize Listing Page ?

How do we implement complex filters in redis ?
How to implement pagination on redis data structures ?

Stay tuned for part 2.

*Just like a good joke, a cache should never be stale. Keep it fresh, keep it fun. Happy coding!* 😄

---

#### [1] What is [the binary log](https://dev.mysql.com/doc/refman/8.0/en/binary-log.html) ? {#WAL}

The binary log contains “events” that describe database changes such as table creation operations or changes to table data. It also contains events for statements that potentially could have made changes (for example, a DELETE which matched no rows), unless row-based logging is used. The binary log also contains information about how long each statement took that updated data.

In PostgresSQL [WAL](https://www.postgresql.org/docs/current/wal-intro.html)

Write-Ahead Logging (WAL) is a standard method for ensuring data integrity. A detailed description can be found in most (if not all) books about transaction processing. Briefly, WAL's central concept is that changes to data files (where tables and indexes reside) must be written only after those changes have been logged, that is, after WAL records describing the changes have been flushed to permanent storage. If we follow this procedure, we do not need to flush data pages to disk on every transaction commit, because we know that in the event of a crash we will be able to recover the database using the log: any changes that have not been applied to the data pages can be redone from the WAL records. (This is roll-forward recovery, also known as REDO.)

Similarly most database platforms provide a way to read database changes.

Data Engineers would be familiar with term - CDC - [Change data capture](https://www.confluent.io/learn/change-data-capture/)

Change data capture (CDC) refers to the tracking of all changes in a data source (databases, data warehouses, etc.) so they can be captured in destination systems. In short, CDC allows organizations to achieve data integrity and consistency across all systems and deployment environments.
