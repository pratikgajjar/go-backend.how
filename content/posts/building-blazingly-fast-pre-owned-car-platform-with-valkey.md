+++
title = "Building blazingly fast pre-owned car platform with Valkey"
description = "You will be able to use valkey like swiss-knife it is to serve web pages at blazingly fast speed."
date = 2024-06-16T23:27:27+05:30
lastmod = 2024-06-16T23:27:27+05:30
draft = false
images = []
+++

**Introduction**

Welcome! This blog post is a work in progress, but feel free to dive in. The content will cover how to leverage [Valkey](https://valkey.io/) for serving web pages at blazingly fast speeds. This guide assumes you have some familiarity with backend technology.

{{< details "What is Valkey?" >}}
Valkey is an open-source, high-performance key/value datastore licensed under BSD. It supports a variety of workloads such as caching, message queues, and can function as a primary database. Valkey can operate as a standalone daemon or in a cluster with options for replication and high availability.

{{</details>}}
It originated as a fork of Redis after Redis changed its license terms. More details on the forking issue can be found [here](https://arstechnica.com/information-technology/2024/04/redis-license-change-and-forking-are-a-mess-that-everybody-can-feel-bad-about/).

# What ?

Our platform for pre-owned cars allows buyers to access detailed reports, schedule test drives, and make purchase decisions. It combines a marketplace and inventory model.

## Marketplace

We provide a platform for buyers and sellers to communicate directly. Revenue is generated by facilitating buyer-seller interactions and charging for contact details.

## Inventory

We own the cars and provide test drives at various locations. Revenue comes from higher margins on refurbished cars.

## User flow

The typical user journey involves

1. Viewing ads
2. Generating a lead by entering a mobile number
3. Booking a visit on the platform
4. Test driving at a store
5. Making a purchase decision

Our goal is to maximize lead generation by providing a fast and seamless user experience.

# Why ?

- Aim to achieve a 60ms response time for critical entry points, a target set by our co-founder.
- 95%+ traffic comes from ads on Google, Facebook, Instagram, leading users to either a detailed car page or a listing page.

1. Dedicated Page - Complete details of a single car, featuring over 10 photos and additional information.
2. Listing Page - A catalog of cars with basic information and about four photos that can be scrolled horizontally.

> Amazon found that every 100ms of latency cost them 1% in sales.
In 2006, Google found an extra .5 seconds in search page generation
time dropped traffic by 20%. - [Marissa Mayer](http://glinden.blogspot.com/2006/11/marissa-mayer-at-web-20.html)

# Specifications

- Up to 10,000 active cars available for users to view.
- Listing page shows cars ordered by the [LightFM](https://github.com/lyst/lightfm) model.
- Filters available based on car attributes.
- Handles both anonymous and logged-in users for personalized recommendations.

# Exisitng design

- Frontend: React Progressive Web App (PWA) built using [create-react-app](https://create-react-app.dev/) as the frontend.
- Backend: REST API implemented via [Django](https://www.djangoproject.com/) and Django-Rest-Framework.

## Frontend

PWAs load quickly after the first visit, but the initial load can be slow, especially on low-end devices. However, once loaded, they offer smooth navigation and offline access. We aim to make pages load fast right from the first visit, even on low-end devices.

## Backend

The request-response lifecycle involves multiple steps, with significant time spent on model serialization and database (MySQL) calls.

### Dedicated Page

```curl
GET https://api.car.com/listing/14006824/
```

Fetches car data based on an ID in URI and returns JSON.

Dynamic Entities

- seller_mobile_no - paid customers
- is_seen - car seen earlier
- test_drive_booked - have booked test drive for the given car
- inspection_report - paid customers
- offer_amount - buyer can choose to offer lower amount than listed amount
- interested_people - How many users made bid, or scheduled test drive
- images - List of image URLs, can change after car made live.
- price - Everyday prices may change based on demand-supply.
- rating - Provided by platform based on rules based logic depends on factor - owner, usage, damage

### Listing Page

```curl
1. GET https://api.car.com/listing/?city_id=1&model_id=123&owner=1
2. GET https://api.car.com/listing/?slug=used-tata-nexon-cars-in-mumbai
3. GET https://api.car.com/listing/?slug=used-tata-nexon-cars-in-mumbai&buyer_id=123
4. GET https://api.car.com/listing/?slug=used-tata-cars-in-mumbai&seen_card_ids=10,11,12
```

Data science model can use either `buyer_id` or `seen_car_ids` to order list of cars based on most relevant to least. In case of `None` order based on generic recommendation.

```python
def get_recommendation(active_car_ids: [int], buyer_id: int, seen_card_ids: Optional[int]):
    return model.get_order(active_car_ids, buyer_id=buyer_id, seen_card_ids=seen_card_ids)
```

#### Filters

Filter By - Car colour, Make, Model, Accessory, Rating, Ownership etc.

Implemented with [Django-filter](https://django-filter.readthedocs.io/en/stable/guide/usage.html), complex queries with multiple joins can strain the database.

#### Pagination

We can return list of all cars in mumbai city in one request, but that would put strain on all components involved.

For example for 1000 cars in a city

1. Database - Needs to fetch details of thousand cars
2. Backend server - Convert database response into `JSON`, this would cause high CPU usage
3. Client - Needs to generate `HTML` for thousand cars, browser need to paint the screen.

This would case issues at each stage, to avoid this we return only few set of cars in one request. Among cursor, limit-offset and page-no pagination options we used page-no pagination.

**Page No pagination**

Request

1. `page_size` - client decides how many cars to show, desktop users have more real estate allows to show more cars.
2. `page_no` - based on user's position, client sends page no.

Response

1. `count` - Maximum no of pages client can ask for, after this number client shows `No more results`
2. `next_page` - URL to fetch next page
3. `prev_page` - URL to fetch previous page
2. `results` - List of `JSON` containing basic car information

How do we decide among 2 cars, which one should come before which one ?

To decide that, client can send query param - `order_by` - in backend based on this we order the pages.

Order options

1. reco - Most relevant to least - default order - based on data science model
2. newest - Last added car comes first
3. price_asc - Ascending order of price
4. price_desc - Descending order of price

```
GET https://api.car.com/listing/?city_id=1&order_by=reco&page_no=2&page_size=10
```

Here for recommendation we would be generating order of car in each request.

# Achieving a 60ms response time

## Frontend

## Frontend

### How Does Client-Side Rendering Work?

Client-side rendering (CSR) involves the frontend receiving a JSON payload and using JavaScript to create HTML for displaying data. Libraries like React handle this efficiently by checking if the DOM tree needs updating and only modifying the parts that have changed. However, this process can be slow on low-end devices because React has to generate the HTML, perform a diff, and then update DOM.

### Why Not Send HTML Directly?

Sending pre-rendered HTML, which browsers are optimized to handle, can be more efficient. This is where Next.js (year 2019), becomes valuable. By running a Node server on the backend, Next.js can send HTML for the initial page load. After this, user interactions such as navigating between pages or viewing details fall back to client-side rendering. This hybrid approach offers a smooth, app-like experience.

## Backend

1. Do we need to re-create json for each car ?
2. How can we do minimum amount of work to return the response ?

To keep in mind;

- Every day car prices changes automatically.
- Operation team can make new car live anytime
- To render full car data - it uses ~10 tables from normalised schema
- All changes to db can be done through different portals.
  - Operation portal built using cakephp app - perform operations
  - Inventory App - that talks to different django application
  - Data Science python scripts to update prices in bulk.

### Building Cache Layer

Use case

1. Filter car based on attributes
2. Cache car information

#### Dedicated Page

- Fetch from redis and return
- Cache Miss, fill cache and return

```python
import redis
dedicated_key = "dp:{car_id}"

class DedicatedPageView(generic.ListRetrieveView):
    def get(request: Request, id: int, **kwargs):
      con = redis.get_connection()
      key = dedicated_key.format(car_id=id)
      value = con.get(rkey)
      if not value:
        # cache miss
        car_obj = Car.objects.get(id=id)
        data = DedicatedSerialiser(obj=car_obj).data
        con.set(key, data, ttl=one_day_in_seconds)
        return data
     return value 
# Find the thundering herd aka cache stampede here, # comment with your solution!
```

Are we done ? No

> There are only two hard things in Computer Science: cache invalidation and naming things.
-- [Phil Karlton](https://martinfowler.com/bliki/TwoHardThings.html)

#### Cache Invalidation

In MySQL, you can listen to binlog [[1]](#WAL) and get notified about all changes to the table - DDL & DML

- Data defiantion language
  - CREATE | ALTER | DROP TABLE
- Data modification language
  - INSERT | UPDATE | DELETE | REPLACE FROM TABLE

Using [python library](https://github.com/pratikgajjar/mysql-data-stream-kafka) to hook into mysql change log, we emitted changes to kafka topic.

Topic - Log file where you can only append.

```
Topic Name:

  {database_name}.{table_name}

Messages:

   op: insert | delete | update

   before: json payload 
      with
        key = column name
        value = column value

   after: json payload
```

Using above logic we listen to all topics which would affect final payload generated for dedicated page.

Now here we have 2 choices:

1. Let user fill the cache.
2. Build the page data, fill the cache.

Since we know there could be only 10_000 active cars, we can go ahead with 2nd option as memory usage won't be very high.

Let's say dedicated page payload size was 20KB - for 10_000 cars, we would use only 2 000 000 KB, 2GB memory.

#### Listing Page

How would you implement filter in redis ? What about pagination, can we do that with filter without making DB call ? Stay tuned for part 2

DDL also means Dilwale Dulhaniya Le jaenge, IYKYK

---

#### [1] What is [the binary log](https://dev.mysql.com/doc/refman/8.0/en/binary-log.html) ? {#WAL}

The binary log contains “events” that describe database changes such as table creation operations or changes to table data. It also contains events for statements that potentially could have made changes (for example, a DELETE which matched no rows), unless row-based logging is used. The binary log also contains information about how long each statement took that updated data.

In PostgresSQL [WAL](https://www.postgresql.org/docs/current/wal-intro.html)

Write-Ahead Logging (WAL) is a standard method for ensuring data integrity. A detailed description can be found in most (if not all) books about transaction processing. Briefly, WAL's central concept is that changes to data files (where tables and indexes reside) must be written only after those changes have been logged, that is, after WAL records describing the changes have been flushed to permanent storage. If we follow this procedure, we do not need to flush data pages to disk on every transaction commit, because we know that in the event of a crash we will be able to recover the database using the log: any changes that have not been applied to the data pages can be redone from the WAL records. (This is roll-forward recovery, also known as REDO.)

Similarly most database platforms provide a way to read database changes.

Data Engineers would be familiar with term - CDC - [Change data capture](https://www.confluent.io/learn/change-data-capture/)

Change data capture (CDC) refers to the tracking of all changes in a data source (databases, data warehouses, etc.) so they can be captured in destination systems. In short, CDC allows organizations to achieve data integrity and consistency across all systems and deployment environments.
